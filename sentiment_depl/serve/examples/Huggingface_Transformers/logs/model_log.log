2022-06-29T14:39:23,744 [INFO ] W-9003-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2022-06-29T14:39:23,747 [INFO ] W-9003-sentiments_1.0-stdout MODEL_LOG - [PID]2819
2022-06-29T14:39:23,751 [INFO ] W-9003-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,753 [INFO ] W-9003-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,768 [INFO ] W-9021-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9021
2022-06-29T14:39:23,777 [INFO ] W-9010-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9010
2022-06-29T14:39:23,777 [INFO ] W-9014-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9014
2022-06-29T14:39:23,782 [INFO ] W-9021-sentiments_1.0-stdout MODEL_LOG - [PID]2789
2022-06-29T14:39:23,783 [INFO ] W-9014-sentiments_1.0-stdout MODEL_LOG - [PID]2823
2022-06-29T14:39:23,783 [INFO ] W-9021-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,783 [INFO ] W-9016-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9016
2022-06-29T14:39:23,785 [INFO ] W-9014-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,786 [INFO ] W-9021-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,787 [INFO ] W-9010-sentiments_1.0-stdout MODEL_LOG - [PID]2791
2022-06-29T14:39:23,792 [INFO ] W-9016-sentiments_1.0-stdout MODEL_LOG - [PID]2796
2022-06-29T14:39:23,792 [INFO ] W-9010-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,793 [INFO ] W-9014-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,793 [INFO ] W-9016-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,794 [INFO ] W-9010-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,794 [INFO ] W-9018-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9018
2022-06-29T14:39:23,798 [INFO ] W-9018-sentiments_1.0-stdout MODEL_LOG - [PID]2787
2022-06-29T14:39:23,799 [INFO ] W-9011-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9011
2022-06-29T14:39:23,799 [INFO ] W-9016-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,808 [INFO ] W-9018-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,808 [INFO ] W-9020-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9020
2022-06-29T14:39:23,818 [INFO ] W-9020-sentiments_1.0-stdout MODEL_LOG - [PID]2790
2022-06-29T14:39:23,818 [INFO ] W-9018-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,819 [INFO ] W-9011-sentiments_1.0-stdout MODEL_LOG - [PID]2820
2022-06-29T14:39:23,820 [INFO ] W-9011-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,822 [INFO ] W-9011-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,822 [INFO ] W-9020-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,827 [INFO ] W-9003-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2022-06-29T14:39:23,827 [INFO ] W-9016-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9016.
2022-06-29T14:39:23,830 [INFO ] W-9014-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9014.
2022-06-29T14:39:23,831 [INFO ] W-9010-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9010.
2022-06-29T14:39:23,836 [INFO ] W-9021-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9021.
2022-06-29T14:39:23,836 [INFO ] W-9002-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2022-06-29T14:39:23,837 [INFO ] W-9018-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9018.
2022-06-29T14:39:23,836 [INFO ] W-9020-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,839 [INFO ] W-9002-sentiments_1.0-stdout MODEL_LOG - [PID]2812
2022-06-29T14:39:23,840 [INFO ] W-9002-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,849 [INFO ] W-9002-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,851 [INFO ] W-9020-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9020.
2022-06-29T14:39:23,855 [INFO ] W-9011-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9011.
2022-06-29T14:39:23,863 [INFO ] W-9002-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2022-06-29T14:39:23,878 [INFO ] W-9006-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9006
2022-06-29T14:39:23,879 [INFO ] W-9006-sentiments_1.0-stdout MODEL_LOG - [PID]2816
2022-06-29T14:39:23,880 [INFO ] W-9006-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,881 [INFO ] W-9006-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,911 [INFO ] W-9022-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9022
2022-06-29T14:39:23,911 [INFO ] W-9015-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9015
2022-06-29T14:39:23,911 [INFO ] W-9008-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9008
2022-06-29T14:39:23,911 [INFO ] W-9019-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9019
2022-06-29T14:39:23,917 [INFO ] W-9001-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2022-06-29T14:39:23,912 [INFO ] W-9007-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9007
2022-06-29T14:39:23,918 [INFO ] W-9017-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9017
2022-06-29T14:39:23,927 [INFO ] W-9022-sentiments_1.0-stdout MODEL_LOG - [PID]2794
2022-06-29T14:39:23,927 [INFO ] W-9000-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-29T14:39:23,927 [INFO ] W-9009-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9009
2022-06-29T14:39:23,928 [INFO ] W-9012-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9012
2022-06-29T14:39:23,930 [INFO ] W-9005-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2022-06-29T14:39:23,929 [INFO ] W-9019-sentiments_1.0-stdout MODEL_LOG - [PID]2788
2022-06-29T14:39:23,930 [INFO ] W-9013-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9013
2022-06-29T14:39:23,934 [INFO ] W-9001-sentiments_1.0-stdout MODEL_LOG - [PID]2809
2022-06-29T14:39:23,930 [INFO ] W-9015-sentiments_1.0-stdout MODEL_LOG - [PID]2805
2022-06-29T14:39:23,934 [INFO ] W-9006-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9006.
2022-06-29T14:39:23,937 [INFO ] W-9008-sentiments_1.0-stdout MODEL_LOG - [PID]2824
2022-06-29T14:39:23,935 [INFO ] W-9012-sentiments_1.0-stdout MODEL_LOG - [PID]2814
2022-06-29T14:39:23,938 [INFO ] W-9009-sentiments_1.0-stdout MODEL_LOG - [PID]2793
2022-06-29T14:39:23,937 [INFO ] W-9007-sentiments_1.0-stdout MODEL_LOG - [PID]2803
2022-06-29T14:39:23,938 [INFO ] W-9000-sentiments_1.0-stdout MODEL_LOG - [PID]2792
2022-06-29T14:39:23,937 [INFO ] W-9019-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,940 [INFO ] W-9013-sentiments_1.0-stdout MODEL_LOG - [PID]2822
2022-06-29T14:39:23,934 [INFO ] W-9017-sentiments_1.0-stdout MODEL_LOG - [PID]2807
2022-06-29T14:39:23,940 [INFO ] W-9009-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,939 [INFO ] W-9022-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,940 [INFO ] W-9008-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,940 [INFO ] W-9005-sentiments_1.0-stdout MODEL_LOG - [PID]2817
2022-06-29T14:39:23,940 [INFO ] W-9001-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,942 [INFO ] W-9007-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,942 [INFO ] W-9019-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,942 [INFO ] W-9012-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,943 [INFO ] W-9000-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,943 [INFO ] W-9012-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,944 [INFO ] W-9008-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,944 [INFO ] W-9013-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,945 [INFO ] W-9015-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,945 [INFO ] W-9005-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,954 [INFO ] W-9017-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,958 [INFO ] W-9015-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,954 [INFO ] W-9013-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,959 [INFO ] W-9009-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,966 [INFO ] W-9000-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,960 [INFO ] W-9007-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,968 [INFO ] W-9001-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,970 [INFO ] W-9022-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,971 [INFO ] W-9005-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,973 [INFO ] W-9017-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,976 [INFO ] W-9002-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:23,976 [INFO ] W-9010-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:23,980 [INFO ] W-9004-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2022-06-29T14:39:23,981 [INFO ] W-9016-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:23,985 [INFO ] W-9004-sentiments_1.0-stdout MODEL_LOG - [PID]2818
2022-06-29T14:39:23,986 [INFO ] W-9004-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,987 [INFO ] W-9004-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:23,989 [INFO ] W-9023-sentiments_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9023
2022-06-29T14:39:23,991 [INFO ] W-9011-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:23,992 [INFO ] W-9006-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:23,994 [INFO ] W-9023-sentiments_1.0-stdout MODEL_LOG - [PID]2806
2022-06-29T14:39:23,995 [INFO ] W-9020-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:23,995 [INFO ] W-9023-sentiments_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-29T14:39:23,995 [INFO ] W-9021-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:23,995 [INFO ] W-9018-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:24,000 [INFO ] W-9023-sentiments_1.0-stdout MODEL_LOG - Python runtime: 3.9.13
2022-06-29T14:39:24,002 [INFO ] W-9013-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9013.
2022-06-29T14:39:23,999 [INFO ] W-9014-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:24,005 [INFO ] W-9009-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9009.
2022-06-29T14:39:23,999 [INFO ] W-9003-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:24,006 [INFO ] W-9022-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9022.
2022-06-29T14:39:24,005 [INFO ] W-9017-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9017.
2022-06-29T14:39:24,012 [INFO ] W-9004-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2022-06-29T14:39:24,014 [INFO ] W-9015-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9015.
2022-06-29T14:39:24,010 [INFO ] W-9008-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9008.
2022-06-29T14:39:24,013 [INFO ] W-9019-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9019.
2022-06-29T14:39:24,014 [INFO ] W-9000-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-29T14:39:24,013 [INFO ] W-9005-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2022-06-29T14:39:24,014 [INFO ] W-9001-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2022-06-29T14:39:24,015 [INFO ] W-9023-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9023.
2022-06-29T14:39:24,013 [INFO ] W-9007-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9007.
2022-06-29T14:39:24,010 [INFO ] W-9012-sentiments_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9012.
2022-06-29T14:39:24,077 [INFO ] W-9023-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:24,078 [INFO ] W-9004-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:24,079 [INFO ] W-9013-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:24,094 [INFO ] W-9008-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:24,098 [INFO ] W-9001-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:24,104 [INFO ] W-9019-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:24,104 [INFO ] W-9012-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:24,106 [INFO ] W-9022-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:24,121 [INFO ] W-9009-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:24,121 [INFO ] W-9017-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:24,124 [INFO ] W-9007-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:24,137 [INFO ] W-9015-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:24,137 [INFO ] W-9005-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:24,157 [INFO ] W-9000-sentiments_1.0-stdout MODEL_LOG - model_name: sentiments, batchSize: 1
2022-06-29T14:39:25,527 [INFO ] W-9010-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,591 [INFO ] W-9002-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,610 [INFO ] W-9014-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,623 [INFO ] W-9020-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,634 [INFO ] W-9016-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,678 [INFO ] W-9023-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,690 [INFO ] W-9011-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,700 [INFO ] W-9021-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,709 [INFO ] W-9005-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,721 [INFO ] W-9007-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,732 [INFO ] W-9015-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,776 [INFO ] W-9008-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,784 [INFO ] W-9013-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,794 [INFO ] W-9003-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,805 [INFO ] W-9009-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,815 [INFO ] W-9001-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,825 [INFO ] W-9018-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,883 [INFO ] W-9019-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,904 [INFO ] W-9022-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,925 [INFO ] W-9012-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:25,986 [INFO ] W-9004-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:26,030 [INFO ] W-9017-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:26,133 [INFO ] W-9006-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:26,239 [INFO ] W-9000-sentiments_1.0-stdout MODEL_LOG - generated new fontManager
2022-06-29T14:39:26,729 [INFO ] W-9010-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:26,775 [INFO ] W-9002-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:26,782 [INFO ] W-9016-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:26,828 [INFO ] W-9020-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:26,830 [INFO ] W-9023-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:26,853 [INFO ] W-9021-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:26,867 [INFO ] W-9011-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:26,921 [INFO ] W-9014-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:26,957 [INFO ] W-9015-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:26,974 [INFO ] W-9007-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:26,985 [INFO ] W-9005-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:27,038 [INFO ] W-9013-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:27,049 [INFO ] W-9001-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:27,075 [INFO ] W-9003-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:27,098 [INFO ] W-9009-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:27,106 [INFO ] W-9008-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:27,247 [INFO ] W-9019-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:27,252 [INFO ] W-9018-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:27,257 [INFO ] W-9022-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:27,259 [INFO ] W-9012-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:27,370 [INFO ] W-9004-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:27,451 [INFO ] W-9017-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:27,509 [INFO ] W-9006-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:27,643 [INFO ] W-9000-sentiments_1.0-stdout MODEL_LOG - Transformers version 4.20.1
2022-06-29T14:39:38,114 [INFO ] W-9010-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:38,178 [INFO ] W-9016-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:38,203 [INFO ] W-9002-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:38,396 [INFO ] W-9020-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:38,612 [INFO ] W-9023-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:38,757 [INFO ] W-9021-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:38,972 [INFO ] W-9011-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:39,057 [INFO ] W-9013-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:39,146 [INFO ] W-9014-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:39,204 [INFO ] W-9012-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:39,213 [INFO ] W-9003-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:39,236 [INFO ] W-9008-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:39,327 [INFO ] W-9007-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:39,328 [INFO ] W-9005-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:39,328 [INFO ] W-9009-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:39,338 [INFO ] W-9015-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:39,406 [INFO ] W-9001-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:39,477 [INFO ] W-9006-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:39,500 [INFO ] W-9019-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:39,526 [INFO ] W-9000-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:39,531 [INFO ] W-9004-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:39,624 [INFO ] W-9018-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:39,628 [INFO ] W-9017-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:39:39,630 [INFO ] W-9022-sentiments_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/fbd9234820634bb0af437ac3e85de806 loaded successfully
2022-06-29T14:53:54,955 [INFO ] W-9010-sentiments_1.0-stdout MODEL_LOG - Backend received inference at: 1656514434
2022-06-29T14:53:54,963 [INFO ] W-9010-sentiments_1.0-stdout MODEL_LOG - Received text: 'good
2022-06-29T14:53:54,964 [INFO ] W-9010-sentiments_1.0-stdout MODEL_LOG - '
2022-06-29T14:53:54,975 [WARN ] W-9010-sentiments_1.0-stderr MODEL_LOG - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
2022-06-29T14:53:55,003 [WARN ] W-9010-sentiments_1.0-stderr MODEL_LOG - /usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
2022-06-29T14:53:55,005 [WARN ] W-9010-sentiments_1.0-stderr MODEL_LOG -   warnings.warn(
2022-06-29T14:53:55,864 [INFO ] W-9010-sentiments_1.0-stdout MODEL_LOG - This the output size from the Seq classification model torch.Size([1, 2])
2022-06-29T14:53:55,866 [INFO ] W-9010-sentiments_1.0-stdout MODEL_LOG - This the output from the Seq classification model SequenceClassifierOutput(loss=None, logits=tensor([[-4.1170,  4.4840]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
2022-06-29T14:53:55,867 [INFO ] W-9010-sentiments_1.0-stdout MODEL_LOG - Generated text ['Positive']
2022-06-29T14:55:35,931 [INFO ] W-9016-sentiments_1.0-stdout MODEL_LOG - Backend received inference at: 1656514535
2022-06-29T14:55:35,944 [INFO ] W-9016-sentiments_1.0-stdout MODEL_LOG - Received text: 'bad
2022-06-29T14:55:35,946 [INFO ] W-9016-sentiments_1.0-stdout MODEL_LOG - '
2022-06-29T14:55:35,956 [WARN ] W-9016-sentiments_1.0-stderr MODEL_LOG - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
2022-06-29T14:55:35,998 [WARN ] W-9016-sentiments_1.0-stderr MODEL_LOG - /usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
2022-06-29T14:55:35,999 [WARN ] W-9016-sentiments_1.0-stderr MODEL_LOG -   warnings.warn(
2022-06-29T14:55:36,888 [INFO ] W-9016-sentiments_1.0-stdout MODEL_LOG - This the output size from the Seq classification model torch.Size([1, 2])
2022-06-29T14:55:36,890 [INFO ] W-9016-sentiments_1.0-stdout MODEL_LOG - This the output from the Seq classification model SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6932, -3.7391]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
2022-06-29T14:55:36,891 [INFO ] W-9016-sentiments_1.0-stdout MODEL_LOG - Generated text ['Negative']
2022-06-29T15:00:09,874 [INFO ] W-9002-sentiments_1.0-stdout MODEL_LOG - Backend received inference at: 1656514809
2022-06-29T15:00:09,886 [INFO ] W-9002-sentiments_1.0-stdout MODEL_LOG - Received text: 'I liked this quick tutorial'
2022-06-29T15:00:09,895 [WARN ] W-9002-sentiments_1.0-stderr MODEL_LOG - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
2022-06-29T15:00:09,943 [WARN ] W-9002-sentiments_1.0-stderr MODEL_LOG - /usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
2022-06-29T15:00:09,944 [WARN ] W-9002-sentiments_1.0-stderr MODEL_LOG -   warnings.warn(
2022-06-29T15:00:10,771 [INFO ] W-9002-sentiments_1.0-stdout MODEL_LOG - This the output size from the Seq classification model torch.Size([1, 2])
2022-06-29T15:00:10,773 [INFO ] W-9002-sentiments_1.0-stdout MODEL_LOG - This the output from the Seq classification model SequenceClassifierOutput(loss=None, logits=tensor([[-3.4564,  3.5650]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
2022-06-29T15:00:10,775 [INFO ] W-9002-sentiments_1.0-stdout MODEL_LOG - Generated text ['Positive']
2022-06-29T15:04:56,901 [INFO ] W-9020-sentiments_1.0-stdout MODEL_LOG - Backend received inference at: 1656515096
2022-06-29T15:04:56,919 [INFO ] W-9020-sentiments_1.0-stdout MODEL_LOG - Received text: ''
2022-06-29T15:04:56,932 [WARN ] W-9020-sentiments_1.0-stderr MODEL_LOG - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
2022-06-29T15:04:56,968 [WARN ] W-9020-sentiments_1.0-stderr MODEL_LOG - /usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
2022-06-29T15:04:56,970 [WARN ] W-9020-sentiments_1.0-stderr MODEL_LOG -   warnings.warn(
2022-06-29T15:04:57,759 [INFO ] W-9020-sentiments_1.0-stdout MODEL_LOG - This the output size from the Seq classification model torch.Size([1, 2])
2022-06-29T15:04:57,760 [INFO ] W-9020-sentiments_1.0-stdout MODEL_LOG - This the output from the Seq classification model SequenceClassifierOutput(loss=None, logits=tensor([[-0.4999,  0.5887]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
2022-06-29T15:04:57,762 [INFO ] W-9020-sentiments_1.0-stdout MODEL_LOG - Generated text ['Positive']
2022-06-29T15:05:05,974 [INFO ] W-9023-sentiments_1.0-stdout MODEL_LOG - Backend received inference at: 1656515105
2022-06-29T15:05:05,989 [INFO ] W-9023-sentiments_1.0-stdout MODEL_LOG - Received text: ''
2022-06-29T15:05:05,998 [WARN ] W-9023-sentiments_1.0-stderr MODEL_LOG - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
2022-06-29T15:05:06,036 [WARN ] W-9023-sentiments_1.0-stderr MODEL_LOG - /usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
2022-06-29T15:05:06,038 [WARN ] W-9023-sentiments_1.0-stderr MODEL_LOG -   warnings.warn(
2022-06-29T15:05:06,663 [INFO ] W-9023-sentiments_1.0-stdout MODEL_LOG - This the output size from the Seq classification model torch.Size([1, 2])
2022-06-29T15:05:06,664 [INFO ] W-9023-sentiments_1.0-stdout MODEL_LOG - This the output from the Seq classification model SequenceClassifierOutput(loss=None, logits=tensor([[-0.4999,  0.5887]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
2022-06-29T15:05:06,666 [INFO ] W-9023-sentiments_1.0-stdout MODEL_LOG - Generated text ['Positive']
2022-06-29T15:05:13,712 [INFO ] W-9021-sentiments_1.0-stdout MODEL_LOG - Backend received inference at: 1656515113
2022-06-29T15:05:13,736 [INFO ] W-9021-sentiments_1.0-stdout MODEL_LOG - Received text: ''
2022-06-29T15:05:13,746 [WARN ] W-9021-sentiments_1.0-stderr MODEL_LOG - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
2022-06-29T15:05:13,783 [WARN ] W-9021-sentiments_1.0-stderr MODEL_LOG - /usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
2022-06-29T15:05:13,785 [WARN ] W-9021-sentiments_1.0-stderr MODEL_LOG -   warnings.warn(
2022-06-29T15:05:14,583 [INFO ] W-9021-sentiments_1.0-stdout MODEL_LOG - This the output size from the Seq classification model torch.Size([1, 2])
2022-06-29T15:05:14,585 [INFO ] W-9021-sentiments_1.0-stdout MODEL_LOG - This the output from the Seq classification model SequenceClassifierOutput(loss=None, logits=tensor([[-0.4999,  0.5887]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
2022-06-29T15:05:14,587 [INFO ] W-9021-sentiments_1.0-stdout MODEL_LOG - Generated text ['Positive']
2022-06-29T15:09:47,345 [INFO ] W-9011-sentiments_1.0-stdout MODEL_LOG - Backend received inference at: 1656515387
2022-06-29T15:09:47,361 [INFO ] W-9011-sentiments_1.0-stdout MODEL_LOG - Received text: 'good
2022-06-29T15:09:47,362 [INFO ] W-9011-sentiments_1.0-stdout MODEL_LOG - '
2022-06-29T15:09:47,373 [WARN ] W-9011-sentiments_1.0-stderr MODEL_LOG - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
2022-06-29T15:09:47,405 [WARN ] W-9011-sentiments_1.0-stderr MODEL_LOG - /usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
2022-06-29T15:09:47,406 [WARN ] W-9011-sentiments_1.0-stderr MODEL_LOG -   warnings.warn(
2022-06-29T15:09:48,092 [INFO ] W-9011-sentiments_1.0-stdout MODEL_LOG - This the output size from the Seq classification model torch.Size([1, 2])
2022-06-29T15:09:48,093 [INFO ] W-9011-sentiments_1.0-stdout MODEL_LOG - This the output from the Seq classification model SequenceClassifierOutput(loss=None, logits=tensor([[-4.1170,  4.4840]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
2022-06-29T15:09:48,095 [INFO ] W-9011-sentiments_1.0-stdout MODEL_LOG - Generated text ['Positive']
2022-06-29T15:11:52,781 [INFO ] W-9013-sentiments_1.0-stdout MODEL_LOG - Backend received inference at: 1656515512
2022-06-29T15:11:52,791 [INFO ] W-9013-sentiments_1.0-stdout MODEL_LOG - Received text: 'I liked this quick tutorial'
2022-06-29T15:11:52,803 [WARN ] W-9013-sentiments_1.0-stderr MODEL_LOG - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
2022-06-29T15:11:52,831 [WARN ] W-9013-sentiments_1.0-stderr MODEL_LOG - /usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
2022-06-29T15:11:52,832 [WARN ] W-9013-sentiments_1.0-stderr MODEL_LOG -   warnings.warn(
2022-06-29T15:11:53,299 [INFO ] W-9013-sentiments_1.0-stdout MODEL_LOG - This the output size from the Seq classification model torch.Size([1, 2])
2022-06-29T15:11:53,301 [INFO ] W-9013-sentiments_1.0-stdout MODEL_LOG - This the output from the Seq classification model SequenceClassifierOutput(loss=None, logits=tensor([[-3.4564,  3.5650]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
2022-06-29T15:11:53,303 [INFO ] W-9013-sentiments_1.0-stdout MODEL_LOG - Generated text ['Positive']
